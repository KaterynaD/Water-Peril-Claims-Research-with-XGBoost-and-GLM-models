{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save SageMaker ModelTraining.py as ModelTraining.py (or adjust training_job_entry_point below) in the same folder as this notebook\n",
    "#Save \"SageMaker Prediction.py\" as inference.py in the same folder as this notebook. inference.py is a mandatory name and can not be changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29 µs, sys: 0 ns, total: 29 µs\n",
      "Wall time: 37.2 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "UseSavedIfExists = False\n",
    "#provide models files in s3 if UseSavedIfExists is True\n",
    "#There is a way to check if deployed models exist but \n",
    "#I feel safe to check the actual model file exists\n",
    "#and did no find a way to get the model ref from the list\n",
    "#client = boto3.client('sagemaker') -> list_models() -> get_model???\n",
    "models_data = list()\n",
    "#where are training data located locally\n",
    "DataDir = '/home/kate/Research/Property/Data/'\n",
    "#Temp local dir to save file before moving to S3\n",
    "TmpDir = '/home/kate/Research/Property/Notebooks/SageMaker/tmp/'\n",
    "#training data file name\n",
    "training_dataset_name='property_wcf_class_training_basemodel0'\n",
    "testing_dataset_name='property_wcf_testing'\n",
    "#file name template for each fold to be saved in S3\n",
    "training_dataset_fold_filename='training_dataset_fold'\n",
    "validateion_dataset_fold_filename='validation_dataset_fold'\n",
    "#model target column\n",
    "target_column='hasclaim'\n",
    "#model featureset\n",
    "featureset  = [\n",
    "'roofcd_encd',\n",
    "'sqft',  \n",
    "'usagetype_encd',\n",
    "'yearbuilt',\n",
    "'water_risk_3_blk',\n",
    "'landlordind',\n",
    "'multipolicyind'  \n",
    "]\n",
    "#model parameters\n",
    "hyperparameters = {\n",
    "'objective': 'binary:logistic',\n",
    "        'eval_metric': 'auc',\n",
    "        'silent': True,\n",
    "        'booster': 'gbtree',\n",
    "        'seed': 42,\n",
    "        'scale_pos_weight':0.3,\n",
    "        'colsample_bylevel': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'eta': 0.01,\n",
    "        'max_depth': 6,\n",
    "        'num_round':'5000'\n",
    "}\n",
    "#S3 bucket and folders to save files\n",
    "bucket = 'sagemaker-wc-class'\n",
    "#training fold data saved in\n",
    "data_folder_in_bucket = 'data/folds/'\n",
    "#models saved in\n",
    "model_folder_in_bucket = 'model/fold'\n",
    "#data in training and testing file type\n",
    "content_type='text/csv'\n",
    "#Script to be load in training instance\n",
    "training_job_entry_point='ModelTraining.py'\n",
    "#Training job name. Can be changed but there is a mandatory pattern\n",
    "training_job_name = 'basemodel0-class-XGB'\n",
    "#instance type to be created for training and transformation jobs. \n",
    "#They can be different types if needed\n",
    "instance_type='ml.c5.xlarge'\n",
    "#Transformation jobs\n",
    "#Model is created for each fold model_name is just a template. fold number will be added \n",
    "model_name='property-wcf-class-basemodel0'\n",
    "transformation_job_entry_point='inference.py'\n",
    "transformation_input_folder = 'data/folds'\n",
    "transformation_output_folder = 'output'\n",
    "s3_batch_output = 's3://%s/%s/%s_fold_'%(bucket,transformation_output_folder,model_name)\n",
    "s3_batch_input_training = 's3://%s/%s/%s.csv'%(bucket,transformation_input_folder,training_dataset_name)\n",
    "s3_batch_input_testing = 's3://%s/%s/%s.csv'%(bucket,transformation_input_folder,testing_dataset_name)   \n",
    "#column names with predicted data to add in the datasets\n",
    "prediction_column_cv='sm_basemodel0_class_xgb_cv'\n",
    "prediction_column_fold = 'sm_basemodel0_class_xgb_fold'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 482 ms, sys: 36.2 ms, total: 518 ms\n",
      "Wall time: 517 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#packages used in the notebook\n",
    "import time\n",
    "import sys\n",
    "import boto3\n",
    "import re\n",
    "import sagemaker\n",
    "from sagemaker.session import s3_input\n",
    "from sagemaker.xgboost.estimator import XGBoost\n",
    "from sagemaker.xgboost.model import XGBoostModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34.1 ms, sys: 4.06 ms, total: 38.2 ms\n",
      "Wall time: 37.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#should be run as a first step\n",
    "#role arn is used when run from a local machine\n",
    "role = 'arn:aws:iam::'\n",
    "region = boto3.Session().region_name\n",
    "s3 = s3fs.S3FileSystem()\n",
    "smclient = boto3.Session().client('sagemaker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.34 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#In order to compare training in the local mode and SageMaker \n",
    "#I need to use the same fold splitting in the training data in both modes\n",
    "#The local training script loop thry training dataset in the memory\n",
    "#SageMaker works only with data in S3 bucket\n",
    "#Each training data fold is saved as a file in the bucket\n",
    "#Only requred columns (target and featureset in training for modeling and only featureset for prediction)\n",
    "#No header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.69 s, sys: 304 ms, total: 3.99 s\n",
      "Wall time: 3.99 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#data\n",
    "training_dataset = pd.read_csv('%s%s.csv'%(DataDir,training_dataset_name), error_bad_lines=False, index_col=False)\n",
    "testing_dataset = pd.read_csv('%s%s.csv'%(DataDir,testing_dataset_name), error_bad_lines=False, index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "property_wcf_class_training_basemodel0.csv does not exist in S3. Loading...\n",
      "CPU times: user 1.29 s, sys: 17.1 ms, total: 1.31 s\n",
      "Wall time: 9.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Load training data to S3 bucket\n",
    "#We need the dataset as a whole for prediction and calculating models metrics\n",
    "if not(s3.exists('s3://%s/%s%s.csv'%(bucket,data_folder_in_bucket,training_dataset_name)) & UseSavedIfExists):\n",
    "    print('%s.csv does not exist in S3. Loading...'%training_dataset_name)\n",
    "    training_dataset[featureset].to_csv('s3://%s/%s%s.csv'%(bucket,data_folder_in_bucket,training_dataset_name),header=False,index=False)\n",
    "else:\n",
    "    print('%s.csv exists in S3'%training_dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "property_wcf_testing.csv does not exist in S3. Loading...\n",
      "CPU times: user 1.21 s, sys: 16.5 ms, total: 1.23 s\n",
      "Wall time: 9.83 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Load testing data to S3 bucket\n",
    "#We need the dataset for prediction and calculating models metrics\n",
    "if not(s3.exists('s3://%s/%s%s.csv'%(bucket,data_folder_in_bucket,testing_dataset_name)) & UseSavedIfExists):\n",
    "    print('%s.csv does not exist in S3. Loading...'%testing_dataset_name)\n",
    "    testing_dataset[featureset].to_csv('s3://%s/%s%s.csv'%(bucket,data_folder_in_bucket,testing_dataset_name),header=False,index=False)\n",
    "else:\n",
    "    print('%s.csv exists in S3'%testing_dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 6.2 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Splitting to folds, load to the bucket training data and training\n",
    "#The script does not wait till the end of the training\n",
    "#All training jobs are run in parallel\n",
    "kfold = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " fold: 1  of  5 : \n",
      "training_dataset_fold0.csv does not exist in S3. Loading...\n",
      "validation_dataset_fold0.csv does not exist in S3. Loading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-wc-class/model/fold_0/basemodel0-class-XGB-0-2020-10-29-22-41-41/output/model.tar.gz model does not exists. Training...\n",
      " fold: 2  of  5 : \n",
      "training_dataset_fold1.csv does not exist in S3. Loading...\n",
      "validation_dataset_fold1.csv does not exist in S3. Loading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-wc-class/model/fold_1/basemodel0-class-XGB-1-2020-10-29-22-41-54/output/model.tar.gz model does not exists. Training...\n",
      " fold: 3  of  5 : \n",
      "training_dataset_fold2.csv does not exist in S3. Loading...\n",
      "validation_dataset_fold2.csv does not exist in S3. Loading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-wc-class/model/fold_2/basemodel0-class-XGB-2-2020-10-29-22-42-07/output/model.tar.gz model does not exists. Training...\n",
      " fold: 4  of  5 : \n",
      "training_dataset_fold3.csv does not exist in S3. Loading...\n",
      "validation_dataset_fold3.csv does not exist in S3. Loading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-wc-class/model/fold_3/basemodel0-class-XGB-3-2020-10-29-22-42-19/output/model.tar.gz model does not exists. Training...\n",
      " fold: 5  of  5 : \n",
      "training_dataset_fold4.csv does not exist in S3. Loading...\n",
      "validation_dataset_fold4.csv does not exist in S3. Loading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-wc-class/model/fold_4/basemodel0-class-XGB-4-2020-10-29-22-42-31/output/model.tar.gz model does not exists. Training...\n",
      "CPU times: user 7.72 s, sys: 126 ms, total: 7.84 s\n",
      "Wall time: 1min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "training_jobs = list()\n",
    "estimators = list()\n",
    "for i in range(0,kfold):\n",
    "    print(' fold: {}  of  {} : '.format(i+1, kfold))\n",
    "    #Preparing fold data in s3\n",
    "    #training data\n",
    "    if not(s3.exists('s3://%s/%s%s%s.csv'%(bucket,data_folder_in_bucket,training_dataset_fold_filename,i)) & UseSavedIfExists):\n",
    "        print('%s%s.csv does not exist in S3. Loading...'%(training_dataset_fold_filename,i))\n",
    "        training_dataset[training_dataset['fold_%s'%i]>0][[target_column] + featureset].to_csv('s3://%s/%s%s%s.csv'%(bucket,data_folder_in_bucket,training_dataset_fold_filename,i),header=False,index=False)\n",
    "    else:\n",
    "        print('%s%s.csv exists in S3.'%(training_dataset_fold_filename,i))\n",
    "    #validation data\n",
    "    if not(s3.exists('s3://%s/%s%s%s.csv'%(bucket,data_folder_in_bucket,validateion_dataset_fold_filename,i)) & UseSavedIfExists):\n",
    "        print('%s%s.csv does not exist in S3. Loading...'%(validateion_dataset_fold_filename,i))    \n",
    "        training_dataset[training_dataset['fold_%s'%i]==0][[target_column] + featureset].to_csv('s3://%s/%s%s%s.csv'%(bucket,data_folder_in_bucket,validateion_dataset_fold_filename,i),header=False,index=False)\n",
    "    else:\n",
    "         print('%s%s.csv exists in S3.'%(validateion_dataset_fold_filename,i)) \n",
    "    #Estimator and training    \n",
    "    if len(models_data)-1>=i:\n",
    "        model_data=models_data[i]\n",
    "    else:\n",
    "        model_data='empty'\n",
    "    if not(s3.exists(model_data) & UseSavedIfExists):\n",
    "        xgb_script_mode_estimator_fold = XGBoost(\n",
    "        entry_point=training_job_entry_point,\n",
    "        hyperparameters=hyperparameters,\n",
    "        role=role, \n",
    "        train_instance_count=1,\n",
    "        train_instance_type=instance_type,\n",
    "        framework_version=\"1.0-1\",\n",
    "        output_path='s3://%s/%s_%s'%(bucket, model_folder_in_bucket,i)\n",
    "        )\n",
    "        estimators.append(xgb_script_mode_estimator_fold)\n",
    "        train_input = s3_input('https://s3-%s.amazonaws.com/%s/%s%s%s.csv'%(region, bucket,data_folder_in_bucket,training_dataset_fold_filename,i), content_type=content_type)\n",
    "        validation_input = s3_input('https://s3-%s.amazonaws.com/%s/%s%s%s.csv'%(region, bucket,data_folder_in_bucket,validateion_dataset_fold_filename,i), content_type=content_type)\n",
    "        #Training\n",
    "        job_name_fold=training_job_name +'-%s-'%i+ time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "        training_jobs.append(job_name_fold)\n",
    "        model_data='s3://%s/%s_%s/%s/output/model.tar.gz'%(bucket, model_folder_in_bucket,i,job_name_fold)\n",
    "        print('%s model does not exists. Training...'%model_data)      \n",
    "        if len(models_data)-1>=i:\n",
    "            models_data[i]=model_data\n",
    "        else:\n",
    "            models_data.append(model_data)\n",
    "        xgb_script_mode_estimator_fold.fit({'train': train_input, 'validation': validation_input}, job_name=job_name_fold, wait=False)\n",
    "    else:\n",
    "        print('%s model exists in S3'%model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training job basemodel0-class-XGB-0-2020-10-29-22-41-41 status: InProgress\n",
      "Training job basemodel0-class-XGB-1-2020-10-29-22-41-54 status: InProgress\n",
      "Training job basemodel0-class-XGB-2-2020-10-29-22-42-07 status: InProgress\n",
      "Training job basemodel0-class-XGB-3-2020-10-29-22-42-19 status: InProgress\n",
      "Training job basemodel0-class-XGB-4-2020-10-29-22-42-31 status: InProgress\n",
      "Continue waiting...\n",
      "Training job basemodel0-class-XGB-0-2020-10-29-22-41-41 status: InProgress\n",
      "Training job basemodel0-class-XGB-1-2020-10-29-22-41-54 status: InProgress\n",
      "Training job basemodel0-class-XGB-2-2020-10-29-22-42-07 status: InProgress\n",
      "Training job basemodel0-class-XGB-3-2020-10-29-22-42-19 status: InProgress\n",
      "Training job basemodel0-class-XGB-4-2020-10-29-22-42-31 status: InProgress\n",
      "Continue waiting...\n",
      "Training job basemodel0-class-XGB-0-2020-10-29-22-41-41 status: InProgress\n",
      "Training job basemodel0-class-XGB-1-2020-10-29-22-41-54 status: InProgress\n",
      "Training job basemodel0-class-XGB-2-2020-10-29-22-42-07 status: InProgress\n",
      "Training job basemodel0-class-XGB-3-2020-10-29-22-42-19 status: InProgress\n",
      "Training job basemodel0-class-XGB-4-2020-10-29-22-42-31 status: InProgress\n",
      "Continue waiting...\n",
      "Training job basemodel0-class-XGB-0-2020-10-29-22-41-41 status: Completed\n",
      "Training job basemodel0-class-XGB-1-2020-10-29-22-41-54 status: Completed\n",
      "Training job basemodel0-class-XGB-2-2020-10-29-22-42-07 status: Completed\n",
      "Training job basemodel0-class-XGB-3-2020-10-29-22-42-19 status: InProgress\n",
      "Training job basemodel0-class-XGB-4-2020-10-29-22-42-31 status: InProgress\n",
      "Continue waiting...\n",
      "All Training Jobs are Completed\n",
      "CPU times: user 4.11 s, sys: 148 ms, total: 4.26 s\n",
      "Wall time: 4min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Waiting till the end of all training jobs\n",
    "#If there are None the waiting cycle will not start\n",
    "#Training process should take 5-6 min\n",
    "#Waiting till complete\n",
    "#We could start prediction when a particular training job is complete\n",
    "check_every_sec=10\n",
    "n = 0\n",
    "print_every_n_output=6\n",
    "#If there are not complete training jobs in 5-6 minutes, it's better to look into logs\n",
    "t = 0\n",
    "minutes_to_wait=10*60/check_every_sec\n",
    "EstimatorsFlg=len(estimators)>0\n",
    "JobsFlg=len(training_jobs)>0\n",
    "while (True & EstimatorsFlg & JobsFlg):\n",
    "    statuses = list()\n",
    "    n = n + 1\n",
    "    for e,j in zip(estimators,training_jobs):\n",
    "        status=e.sagemaker_session.describe_training_job(j)['TrainingJobStatus']\n",
    "        if n==print_every_n_output:\n",
    "            print('Training job %s status: %s'%(j,status))\n",
    "        statuses.append(status)\n",
    "    if 'InProgress' in statuses:\n",
    "        if n==print_every_n_output:\n",
    "            print('Continue waiting...')\n",
    "            n = 0\n",
    "    else:\n",
    "        if set(statuses)=={'Completed'}:\n",
    "            print('All Training Jobs are Completed')\n",
    "        else:\n",
    "            print('Something went wrong.')\n",
    "        break \n",
    "    t = t+1\n",
    "    if t>minutes_to_wait:\n",
    "        print('Something went wrong. Training jobs are still running.')\n",
    "        break\n",
    "    time.sleep(check_every_sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 122 ms, sys: 0 ns, total: 122 ms\n",
      "Wall time: 121 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Evaluation metric to be used in tuning\n",
    "from sklearn.metrics import roc_auc_score,confusion_matrix\n",
    "#To estimate models performance we need a custom gini function\n",
    "def gini(y, pred):\n",
    "    g = np.asarray(np.c_[y, pred, np.arange(len(y)) ], dtype=np.float)\n",
    "    g = g[np.lexsort((g[:,2], -1*g[:,1]))]\n",
    "    gs = g[:,0].cumsum().sum() / g[:,0].sum()\n",
    "    gs -= (len(y) + 1) / 2.\n",
    "    return gs / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models to use in prediction:\n",
      "['s3://sagemaker-wc-class/model/fold_0/basemodel0-class-XGB-0-2020-10-29-22-41-41/output/model.tar.gz', 's3://sagemaker-wc-class/model/fold_1/basemodel0-class-XGB-1-2020-10-29-22-41-54/output/model.tar.gz', 's3://sagemaker-wc-class/model/fold_2/basemodel0-class-XGB-2-2020-10-29-22-42-07/output/model.tar.gz', 's3://sagemaker-wc-class/model/fold_3/basemodel0-class-XGB-3-2020-10-29-22-42-19/output/model.tar.gz', 's3://sagemaker-wc-class/model/fold_4/basemodel0-class-XGB-4-2020-10-29-22-42-31/output/model.tar.gz']\n",
      "CPU times: user 143 µs, sys: 0 ns, total: 143 µs\n",
      "Wall time: 122 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Prediction on training dataset with a transformation job\n",
    "#I can only start 5 transformations jobs at a time\n",
    "#so separate loops for training and testing datasets\n",
    "#XGBoostModel for prediction can be created directly from xgb_script_mode_estimator_fold\n",
    "#But because I can train models and run predict in different runs and \n",
    "#instead of traing just use already created model files\n",
    "#I create XGBoostModel based on this files\n",
    "#models_data list should be populated manualy\n",
    "#Of course, models can be created once and reused\n",
    "print('Models to use in prediction:')\n",
    "print(models_data)\n",
    "#There is a function because there is teh same process for training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "property-wcf-class-basemodel0-0 model does not exist\n",
      "property-wcf-class-basemodel0-0 model was created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "property-wcf-class-basemodel0-1 model was deleted\n",
      "property-wcf-class-basemodel0-1 model was created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "property-wcf-class-basemodel0-2 model was deleted\n",
      "property-wcf-class-basemodel0-2 model was created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "property-wcf-class-basemodel0-3 model was deleted\n",
      "property-wcf-class-basemodel0-3 model was created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "property-wcf-class-basemodel0-4 model was deleted\n",
      "property-wcf-class-basemodel0-4 model was created\n",
      "CPU times: user 215 ms, sys: 20.2 ms, total: 235 ms\n",
      "Wall time: 10.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#The same models are used for prediction based on training and testing data\n",
    "models = list()\n",
    "i = 0\n",
    "for m in models_data:    \n",
    "    #Try to delete if exists model and create a new model based on a model file\n",
    "    name=model_name+'-%s'%i\n",
    "    try:\n",
    "        response = smclient.delete_model(ModelName=name)\n",
    "        print('%s model was deleted'%name)\n",
    "    except:\n",
    "        print('%s model does not exist'%name)\n",
    "        pass\n",
    "    xgb_inference_model = XGBoostModel(\n",
    "    name=name,\n",
    "    model_data=m,\n",
    "    role=role,\n",
    "    entry_point=transformation_job_entry_point,\n",
    "    framework_version=\"1.0-1\",\n",
    "    )\n",
    "    models.append(xgb_inference_model)\n",
    "    print('%s model was created'%name)\n",
    "    i = i + 1        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 6.91 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def RunModelsTransformJobs(s3_batch_input):\n",
    "    tranform_jobs = list()\n",
    "    tranformers = list()\n",
    "    i = 0\n",
    "    for m in models:       \n",
    "        #Create transform job\n",
    "        s3_batch_output_fold=s3_batch_output+'%s'%i\n",
    "        transformer =  m.transformer(\n",
    "                                              instance_count=1, \n",
    "                                              instance_type=instance_type,\n",
    "                                              output_path=s3_batch_output_fold,\n",
    "                                              accept='text/csv',\n",
    "                                              strategy='MultiRecord',\n",
    "                                              assemble_with='Line'\n",
    "                                             )\n",
    "        tranformers.append(transformer)\n",
    "        transformer.transform(data=s3_batch_input, content_type='text/csv',split_type='Line')\n",
    "        job_name = transformer.latest_transform_job.name\n",
    "        tranform_jobs.append(job_name)\n",
    "        print('Job %s started'%job_name)\n",
    "        i = i + 1\n",
    "    return (tranformers,tranform_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
      "Wall time: 6.91 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def WaitForTransformJobs(tranformers, tranform_jobs, check_every_sec,print_every_n_output):\n",
    "    #If there are not complete training jobs in 5-6 minutes, it's better to look into logs\n",
    "    t = 0\n",
    "    n = 0\n",
    "    minutes_to_wait=6*60/check_every_sec\n",
    "    while True:\n",
    "        statuses = list()\n",
    "        n = n + 1\n",
    "        for e,j in zip(tranformers,tranform_jobs):\n",
    "            status=e.sagemaker_session.describe_transform_job(j)['TransformJobStatus']\n",
    "            if n==print_every_n_output:\n",
    "                print('Transform job %s status: %s'%(j,status))\n",
    "            statuses.append(status)\n",
    "        if 'InProgress' in statuses:\n",
    "            if n==print_every_n_output:\n",
    "                print('Continue waiting...')\n",
    "                n = 0\n",
    "        else:\n",
    "            if set(statuses)=={'Completed'}:\n",
    "                print('All Transform Jobs are Completed')\n",
    "            else:\n",
    "                print('Something went wrong.')\n",
    "            break \n",
    "        t = t+1\n",
    "        if t>minutes_to_wait:\n",
    "            print('Something went wrong. Transform jobs are still running.')\n",
    "            break\n",
    "        time.sleep(check_every_sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job property-wcf-class-basemodel0-0-2020-10-29-23-03-44-991 started\n",
      "Job property-wcf-class-basemodel0-1-2020-10-29-23-03-48-574 started\n",
      "Job property-wcf-class-basemodel0-2-2020-10-29-23-03-50-197 started\n",
      "Job property-wcf-class-basemodel0-3-2020-10-29-23-03-51-883 started\n",
      "Job property-wcf-class-basemodel0-4-2020-10-29-23-03-53-886 started\n",
      "Transform job property-wcf-class-basemodel0-0-2020-10-29-23-03-44-991 status: InProgress\n",
      "Transform job property-wcf-class-basemodel0-1-2020-10-29-23-03-48-574 status: InProgress\n",
      "Transform job property-wcf-class-basemodel0-2-2020-10-29-23-03-50-197 status: InProgress\n",
      "Transform job property-wcf-class-basemodel0-3-2020-10-29-23-03-51-883 status: InProgress\n",
      "Transform job property-wcf-class-basemodel0-4-2020-10-29-23-03-53-886 status: InProgress\n",
      "Continue waiting...\n",
      "Transform job property-wcf-class-basemodel0-0-2020-10-29-23-03-44-991 status: InProgress\n",
      "Transform job property-wcf-class-basemodel0-1-2020-10-29-23-03-48-574 status: InProgress\n",
      "Transform job property-wcf-class-basemodel0-2-2020-10-29-23-03-50-197 status: InProgress\n",
      "Transform job property-wcf-class-basemodel0-3-2020-10-29-23-03-51-883 status: InProgress\n",
      "Transform job property-wcf-class-basemodel0-4-2020-10-29-23-03-53-886 status: InProgress\n",
      "Continue waiting...\n",
      "Transform job property-wcf-class-basemodel0-0-2020-10-29-23-03-44-991 status: InProgress\n",
      "Transform job property-wcf-class-basemodel0-1-2020-10-29-23-03-48-574 status: InProgress\n",
      "Transform job property-wcf-class-basemodel0-2-2020-10-29-23-03-50-197 status: InProgress\n",
      "Transform job property-wcf-class-basemodel0-3-2020-10-29-23-03-51-883 status: InProgress\n",
      "Transform job property-wcf-class-basemodel0-4-2020-10-29-23-03-53-886 status: InProgress\n",
      "Continue waiting...\n",
      "Transform job property-wcf-class-basemodel0-0-2020-10-29-23-03-44-991 status: Completed\n",
      "Transform job property-wcf-class-basemodel0-1-2020-10-29-23-03-48-574 status: Completed\n",
      "Transform job property-wcf-class-basemodel0-2-2020-10-29-23-03-50-197 status: Completed\n",
      "Transform job property-wcf-class-basemodel0-3-2020-10-29-23-03-51-883 status: Completed\n",
      "Transform job property-wcf-class-basemodel0-4-2020-10-29-23-03-53-886 status: Completed\n",
      "All Transform Jobs are Completed\n",
      "CPU times: user 4.69 s, sys: 222 ms, total: 4.91 s\n",
      "Wall time: 4min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "training_tranformers,training_tranform_jobs = RunModelsTransformJobs(s3_batch_input_training)\n",
    "WaitForTransformJobs(training_tranformers,training_tranform_jobs, 10,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using already existing model: property-wcf-class-basemodel0-0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job property-wcf-class-basemodel0-0-2020-10-29-23-08-06-018 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using already existing model: property-wcf-class-basemodel0-1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job property-wcf-class-basemodel0-1-2020-10-29-23-08-09-336 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using already existing model: property-wcf-class-basemodel0-2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job property-wcf-class-basemodel0-2-2020-10-29-23-08-10-726 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using already existing model: property-wcf-class-basemodel0-3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job property-wcf-class-basemodel0-3-2020-10-29-23-08-13-767 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using already existing model: property-wcf-class-basemodel0-4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job property-wcf-class-basemodel0-4-2020-10-29-23-08-14-896 started\n",
      "Transform job property-wcf-class-basemodel0-0-2020-10-29-23-08-06-018 status: InProgress\n",
      "Transform job property-wcf-class-basemodel0-1-2020-10-29-23-08-09-336 status: InProgress\n",
      "Transform job property-wcf-class-basemodel0-2-2020-10-29-23-08-10-726 status: InProgress\n",
      "Transform job property-wcf-class-basemodel0-3-2020-10-29-23-08-13-767 status: InProgress\n",
      "Transform job property-wcf-class-basemodel0-4-2020-10-29-23-08-14-896 status: InProgress\n",
      "Continue waiting...\n",
      "Transform job property-wcf-class-basemodel0-0-2020-10-29-23-08-06-018 status: InProgress\n",
      "Transform job property-wcf-class-basemodel0-1-2020-10-29-23-08-09-336 status: InProgress\n",
      "Transform job property-wcf-class-basemodel0-2-2020-10-29-23-08-10-726 status: InProgress\n",
      "Transform job property-wcf-class-basemodel0-3-2020-10-29-23-08-13-767 status: InProgress\n",
      "Transform job property-wcf-class-basemodel0-4-2020-10-29-23-08-14-896 status: InProgress\n",
      "Continue waiting...\n",
      "Transform job property-wcf-class-basemodel0-0-2020-10-29-23-08-06-018 status: InProgress\n",
      "Transform job property-wcf-class-basemodel0-1-2020-10-29-23-08-09-336 status: InProgress\n",
      "Transform job property-wcf-class-basemodel0-2-2020-10-29-23-08-10-726 status: InProgress\n",
      "Transform job property-wcf-class-basemodel0-3-2020-10-29-23-08-13-767 status: InProgress\n",
      "Transform job property-wcf-class-basemodel0-4-2020-10-29-23-08-14-896 status: InProgress\n",
      "Continue waiting...\n",
      "Transform job property-wcf-class-basemodel0-0-2020-10-29-23-08-06-018 status: Completed\n",
      "Transform job property-wcf-class-basemodel0-1-2020-10-29-23-08-09-336 status: Completed\n",
      "Transform job property-wcf-class-basemodel0-2-2020-10-29-23-08-10-726 status: Completed\n",
      "Transform job property-wcf-class-basemodel0-3-2020-10-29-23-08-13-767 status: Completed\n",
      "Transform job property-wcf-class-basemodel0-4-2020-10-29-23-08-14-896 status: InProgress\n",
      "Continue waiting...\n",
      "All Transform Jobs are Completed\n",
      "CPU times: user 4.18 s, sys: 147 ms, total: 4.33 s\n",
      "Wall time: 4min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "testing_tranformers,testing_tranform_jobs = RunModelsTransformJobs(s3_batch_input_testing)\n",
    "WaitForTransformJobs(testing_tranformers,testing_tranform_jobs, 10,6)                                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading predicted data fold: 1  of  5 : \n",
      "Reading predicted data fold: 2  of  5 : \n",
      "Reading predicted data fold: 3  of  5 : \n",
      "Reading predicted data fold: 4  of  5 : \n",
      "Reading predicted data fold: 5  of  5 : \n",
      "CPU times: user 1.64 s, sys: 223 ms, total: 1.87 s\n",
      "Wall time: 9.99 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "training_dataset[prediction_column_cv]=0\n",
    "testing_dataset[prediction_column_cv]=0\n",
    "for i in range(0,kfold):\n",
    "    print('Reading predicted data fold: {}  of  {} : '.format(i+1, kfold))\n",
    "    #training\n",
    "    sm_predicted_data= pd.read_csv('s3://%s/%s/%s_fold_%s/%s.csv.out'%(bucket,transformation_output_folder,model_name,i,training_dataset_name), names=['%s_%s'%(prediction_column_fold,i)], error_bad_lines=False, index_col=False)\n",
    "    training_dataset['%s_%s'%(prediction_column_fold,i)] = sm_predicted_data['%s_%s'%(prediction_column_fold,i)].values\n",
    "    #average\n",
    "    training_dataset[prediction_column_cv]+=   training_dataset['%s_%s'%(prediction_column_fold,i)]/(kfold)\n",
    "    #testing\n",
    "    sm_predicted_data= pd.read_csv('s3://%s/%s/%s_fold_%s/%s.csv.out'%(bucket,transformation_output_folder,model_name,i,testing_dataset_name), names=['%s_%s'%(prediction_column_fold,i)], error_bad_lines=False, index_col=False)\n",
    "    testing_dataset['%s_%s'%(prediction_column_fold,i)] = sm_predicted_data['%s_%s'%(prediction_column_fold,i)].values\n",
    "    #average\n",
    "    testing_dataset[prediction_column_cv]+=   testing_dataset['%s_%s'%(prediction_column_fold,i)]/(kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 196 ms, sys: 0 ns, total: 196 ms\n",
      "Wall time: 195 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Scores cv folds\n",
    "#\n",
    "Train_Gini_l = list()\n",
    "Test_Gini_l = list()\n",
    "Train_ROC_l = list()\n",
    "Test_ROC_l = list()\n",
    "#\n",
    "Train_Gini_l.append(gini(training_dataset[target_column],training_dataset[prediction_column_cv])/gini(training_dataset[target_column],training_dataset[target_column]))\n",
    "Test_Gini_l.append(gini(testing_dataset[target_column],testing_dataset[prediction_column_cv])/gini(testing_dataset[target_column],testing_dataset[target_column]))\n",
    "Train_ROC_l.append(roc_auc_score(training_dataset[target_column], training_dataset[prediction_column_cv]))\n",
    "Test_ROC_l.append(roc_auc_score(testing_dataset[target_column], testing_dataset[prediction_column_cv]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 628 µs, sys: 0 ns, total: 628 µs\n",
      "Wall time: 580 µs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train_Gini</th>\n",
       "      <th>Test_Gini</th>\n",
       "      <th>Train_ROC_AUC</th>\n",
       "      <th>Test_ROC_AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.431514</td>\n",
       "      <td>0.346827</td>\n",
       "      <td>0.715759</td>\n",
       "      <td>0.673413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Train_Gini  Test_Gini  Train_ROC_AUC  Test_ROC_AUC\n",
       "0    0.431514   0.346827       0.715759      0.673413"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "Scores = pd.DataFrame(list(zip(Train_Gini_l,Test_Gini_l,Train_ROC_l,Test_ROC_l)), \n",
    "               columns =['Train_Gini', 'Test_Gini','Train_ROC_AUC', 'Test_ROC_AUC']) \n",
    "Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32.8 s, sys: 393 ms, total: 33.1 s\n",
      "Wall time: 33.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Saving new predictions locally\n",
    "training_dataset.to_csv('%s%s.csv'%(DataDir,training_dataset_name),header=True,index=False)\n",
    "testing_dataset.to_csv('%s%s.csv'%(DataDir,testing_dataset_name),header=True,index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
