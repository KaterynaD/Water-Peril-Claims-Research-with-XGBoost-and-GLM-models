{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save \"SageMaker Prediction with Shap Values.py\" as inference.py in the same folder as this notebook.\n",
    "#inference.py is a mandatory name and can not be changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "models_data = ['s3://sagemaker-wc-class/model/fold_0/basemodel0-class-XGB-0-2020-10-29-22-41-41/output/model.tar.gz', \n",
    "             's3://sagemaker-wc-class/model/fold_1/basemodel0-class-XGB-1-2020-10-29-22-41-54/output/model.tar.gz', \n",
    "             's3://sagemaker-wc-class/model/fold_2/basemodel0-class-XGB-2-2020-10-29-22-42-07/output/model.tar.gz', \n",
    "             's3://sagemaker-wc-class/model/fold_3/basemodel0-class-XGB-3-2020-10-29-22-42-19/output/model.tar.gz', \n",
    "             's3://sagemaker-wc-class/model/fold_4/basemodel0-class-XGB-4-2020-10-29-22-42-31/output/model.tar.gz']\n",
    "#where are training data located locally\n",
    "DataDir = '/home/kate/Research/Property/Data/'\n",
    "#Temp local dir to save file before moving to S3\n",
    "TmpDir = '/home/kate/Research/Property/Notebooks/SageMaker/tmp/'\n",
    "#dataset file name with features to predict\n",
    "dataset_name='property_water_claims_non_cat_fs'\n",
    "#model featureset\n",
    "featureset  = [\n",
    "'roofcd_encd',\n",
    "'sqft',  \n",
    "'usagetype_encd',\n",
    "'yearbuilt',\n",
    "'water_risk_3_blk',\n",
    "'landlordind',\n",
    "'multipolicyind'  \n",
    "]\n",
    "#column names for predicted values\n",
    "prediction_column_cv='sm_basemodel0_class_xgb_cv'\n",
    "transform_output_with_shapvalues  = [\n",
    "'sm_basemodel0_class_xgb_fold',\n",
    "'roofcd_encd_shap_value',\n",
    "'sqft_shap_value',  \n",
    "'usagetype_encd_shap_value',\n",
    "'yearbuilt_shap_value',\n",
    "'water_risk_3_blk_shap_value',\n",
    "'landlordind_shap_value',\n",
    "'multipolicyind_shap_value',\n",
    "'expected_value'\n",
    "]\n",
    "#other columns to join from the original dataset\n",
    "other_columns=[\n",
    "    'modeldata_id',\n",
    "    'cal_year'\n",
    "]\n",
    "#instance type to be created for transformation jobs. \n",
    "instance_type='ml.c5.xlarge'\n",
    "#Transformation jobs\n",
    "#Model is created for each fold model_name is just a template. fold number will be added \n",
    "transformation_job_entry_point='inference.py'\n",
    "#timeout for waiting Shap Values\n",
    "transformation_job_timeout = 3600\n",
    "transformation_input_folder = 'input'\n",
    "transformation_output_folder = 'output'\n",
    "model_name='property-wcf-class-basemodel0'\n",
    "#S3 bucket and folders to save files\n",
    "bucket = 'sagemaker-wc-class'\n",
    "s3_batch_output = 's3://%s/%s/%s_fold_'%(bucket,transformation_output_folder,model_name)\n",
    "s3_batch_input = 's3://%s/%s/%s.csv'%(bucket,transformation_input_folder,dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#packages used in the notebook\n",
    "import time\n",
    "import sys\n",
    "import boto3\n",
    "import re\n",
    "import sagemaker\n",
    "from sagemaker.session import s3_input\n",
    "from sagemaker.xgboost.model import XGBoostModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#should be run as a first step\n",
    "#role arn is used when run from a local machine\n",
    "role = 'arn:aws:iam::'\n",
    "region = boto3.Session().region_name\n",
    "s3 = s3fs.S3FileSystem()\n",
    "smclient = boto3.Session().client('sagemaker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#data\n",
    "dataset = pd.read_csv('%s%s.csv'%(DataDir,dataset_name), error_bad_lines=False, index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Load data to S3 bucket\n",
    "if not(s3.exists(s3_batch_input)):\n",
    "    print('%s.csv does not exist in S3. Loading...'%dataset_name)\n",
    "    dataset[featureset].to_csv(s3_batch_input,header=False,index=False)\n",
    "else:\n",
    "    print('%s.csv exists in S3'%dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Models to be used in prediction\n",
    "#based on model files provided in models_data\n",
    "models = list()\n",
    "i = 0\n",
    "for m in models_data:    \n",
    "    #Try to delete if exists model and create a new model based on a model file\n",
    "    name=model_name+'-%s'%i\n",
    "    try:\n",
    "        response = smclient.delete_model(ModelName=name)\n",
    "        print('%s model was deleted'%name)\n",
    "    except:\n",
    "        print('%s model does not exist'%name)\n",
    "        pass\n",
    "    xgb_inference_model = XGBoostModel(\n",
    "    name=name,\n",
    "    model_data=m,\n",
    "    role=role,\n",
    "    entry_point=transformation_job_entry_point,\n",
    "    framework_version=\"1.0-1\",\n",
    "    )\n",
    "    models.append(xgb_inference_model)\n",
    "    print('%s model was created'%name)\n",
    "    i = i + 1        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "tranform_jobs = list()\n",
    "tranformers = list()\n",
    "i = 0\n",
    "for m in models:       \n",
    "#Create transform job\n",
    "    s3_batch_output_fold=s3_batch_output+'%s'%i\n",
    "    transformer =  m.transformer(\n",
    "                                              instance_count=1, \n",
    "                                              instance_type=instance_type,\n",
    "                                              output_path=s3_batch_output_fold,\n",
    "                                              accept='text/csv',\n",
    "                                              strategy='MultiRecord',\n",
    "                                              assemble_with='Line'\n",
    "                                             ,env = {'SAGEMAKER_MODEL_SERVER_TIMEOUT' : str(transformation_job_timeout) }\n",
    "                                             )\n",
    "    tranformers.append(transformer)\n",
    "    transformer.transform(data=s3_batch_input, content_type='text/csv',split_type='Line')\n",
    "    job_name = transformer.latest_transform_job.name\n",
    "    tranform_jobs.append(job_name)\n",
    "    print('Job %s started'%job_name)\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "#If there are not complete training jobs in 60 minutes, it's better to look into logs. Took 53 min 1 job for ~2M rows\n",
    "check_every_sec=60\n",
    "print_every_n_output=6\n",
    "t = 0\n",
    "n = 0\n",
    "minutes_to_wait=60*60/check_every_sec\n",
    "while True:\n",
    "    statuses = list()\n",
    "    n = n + 1\n",
    "    for e,j in zip(tranformers,tranform_jobs):\n",
    "        status=e.sagemaker_session.describe_transform_job(j)['TransformJobStatus']\n",
    "        if n==print_every_n_output:\n",
    "            print('Transform job %s status: %s'%(j,status))\n",
    "        statuses.append(status)\n",
    "    if 'InProgress' in statuses:\n",
    "        if n==print_every_n_output:\n",
    "            print('Continue waiting...')\n",
    "            n = 0\n",
    "    else:\n",
    "        if set(statuses)=={'Completed'}:\n",
    "            print('All Transform Jobs are Completed')\n",
    "        else:\n",
    "            print('Something went wrong.')\n",
    "        break \n",
    "    t = t+1\n",
    "    if t>minutes_to_wait:\n",
    "        print('Something went wrong. Transform jobs are still running.')\n",
    "        break\n",
    "    time.sleep(check_every_sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "i = 0\n",
    "kfold=len(models)\n",
    "for m in models:   \n",
    "    print('Reading predicted data fold: {}  of  {} : '.format(i+1, kfold))\n",
    "    s3_batch_output_fold=s3_batch_output+'%s/%s.csv.out'%(i,dataset_name)\n",
    "    SageMakerPrediction_dataset = pd.read_csv(s3_batch_output_fold, names=transform_output_with_shapvalues, error_bad_lines=False, index_col=False) \n",
    "    SageMakerPrediction_dataset['ModelName']=model_name\n",
    "    SageMakerPrediction_dataset['fold']=i\n",
    "    #other columns from the original dataset\n",
    "    for f in featureset:\n",
    "        SageMakerPrediction_dataset[f]=dataset[f]\n",
    "    SageMakerPrediction_dataset = SageMakerPrediction_dataset[['ModelName','fold']+featureset+transform_output_with_shapvalues]\n",
    "    SageMakerPrediction_dataset.to_csv('%s%s_%s_%s_sage_maker_shap_values.csv'%(DataDir,dataset_name,model_name,i),header=True,index=False)\n",
    "    i = i+1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
